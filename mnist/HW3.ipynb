{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST \n",
    "\n",
    "## Load up the Tensorflow-gpu version of Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    batch_size is how many iterations will pass before the weights are adjusted\n",
    "    num_classes would be the 10 classes of digits\n",
    "    epochs is how many passes of the data (forward and backward) that we want to make\n",
    "    num_channel is the # of color layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "num_channels = 1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "\n",
    "def convert_images(raw):\n",
    "    global img_rows\n",
    "    global img_cols\n",
    "    global num_channels\n",
    "    images = raw.reshape([-1, num_channels, img_rows, img_cols])\n",
    "    # Reorder the indices of the array.\n",
    "    images = images.transpose([0, 2, 3, 1])\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up the csv data and reshape it\n",
    "\n",
    "### Also normalize the pixel data by diving it by 255.  This will make sure the pixel values are between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (33600, 28, 28, 1)\n",
      "33600 train samples\n",
      "8400 test samples\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "# the first character in the train set is the label, break it off\n",
    "labels = train.iloc[:, 0]\n",
    "features = train.iloc[:, 1:785]\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test /= 255\n",
    "kaggleTest = test.values.reshape(test.shape[0], 28, 28, 1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features.values, labels.values, test_size=0.2, random_state=1212)\n",
    "\n",
    "x_train = convert_images(x_train)\n",
    "x_test = convert_images(x_test)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encode the Labels\n",
    "\n",
    "keras.losses.categorical_crossentrop expects this to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Sequential means the layers are sequential executed\n",
    "    Dense will create a layer of x outputs.  I optionally specify the activation function.  Default activation is linear.\n",
    "    Dropout will raondomly drop some input data to help prevent overfitting.\n",
    "    Flatten will take a multidimensional array and make it one dimensional.\n",
    "    MaxPooling2D, pool_size will downscale the input for a factor of (x,y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\agust\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\agust\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(img_rows, img_cols, num_channels)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "Using Adadelta. From Keras:\n",
    "\n",
    "    Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done. Compared to Adagrad, in the original version of Adadelta you don't have to set an initial learning rate. In this version, initial learning rate and decay factor can be set, as in most other Keras optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Options\n",
    "\n",
    "I need this below to help prevent my GPU from running out of memory.  Otherwise, Python might try to grab all the memory it could up front, and then cause itself not to have enough when it was time to make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "WARNING:tensorflow:From C:\\Users\\agust\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "33600/33600 [==============================] - 5s 141us/sample - loss: 0.3570 - acc: 0.8902 - val_loss: 0.0884 - val_acc: 0.9725\n",
      "Epoch 2/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.1131 - acc: 0.9651 - val_loss: 0.0759 - val_acc: 0.9760\n",
      "Epoch 3/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0816 - acc: 0.9760 - val_loss: 0.0559 - val_acc: 0.9826\n",
      "Epoch 4/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0638 - acc: 0.9796 - val_loss: 0.0459 - val_acc: 0.9873\n",
      "Epoch 5/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0563 - acc: 0.9824 - val_loss: 0.0445 - val_acc: 0.9875\n",
      "Epoch 6/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0430 - acc: 0.9869 - val_loss: 0.0430 - val_acc: 0.9888\n",
      "Epoch 7/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0441 - acc: 0.9862 - val_loss: 0.0384 - val_acc: 0.9888\n",
      "Epoch 8/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0381 - acc: 0.9878 - val_loss: 0.0362 - val_acc: 0.9904\n",
      "Epoch 9/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0338 - acc: 0.9901 - val_loss: 0.0441 - val_acc: 0.9881\n",
      "Epoch 10/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0304 - acc: 0.9903 - val_loss: 0.0384 - val_acc: 0.9890\n",
      "Epoch 11/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0286 - acc: 0.9910 - val_loss: 0.0399 - val_acc: 0.9893\n",
      "Epoch 12/50\n",
      "33600/33600 [==============================] - 3s 91us/sample - loss: 0.0267 - acc: 0.9912 - val_loss: 0.0354 - val_acc: 0.9907\n",
      "Epoch 13/50\n",
      "33600/33600 [==============================] - 3s 91us/sample - loss: 0.0245 - acc: 0.9919 - val_loss: 0.0408 - val_acc: 0.9895\n",
      "Epoch 14/50\n",
      "33600/33600 [==============================] - 3s 91us/sample - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0380 - val_acc: 0.9895\n",
      "Epoch 15/50\n",
      "33600/33600 [==============================] - 3s 91us/sample - loss: 0.0209 - acc: 0.9934 - val_loss: 0.0407 - val_acc: 0.9899\n",
      "Epoch 16/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0194 - acc: 0.9936 - val_loss: 0.0425 - val_acc: 0.9906\n",
      "Epoch 17/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0185 - acc: 0.9937 - val_loss: 0.0451 - val_acc: 0.9898\n",
      "Epoch 18/50\n",
      "33600/33600 [==============================] - 3s 91us/sample - loss: 0.0174 - acc: 0.9944 - val_loss: 0.0407 - val_acc: 0.9912\n",
      "Epoch 19/50\n",
      "33600/33600 [==============================] - 3s 91us/sample - loss: 0.0182 - acc: 0.9940 - val_loss: 0.0432 - val_acc: 0.9901\n",
      "Epoch 20/50\n",
      "33600/33600 [==============================] - 3s 91us/sample - loss: 0.0167 - acc: 0.9947 - val_loss: 0.0354 - val_acc: 0.9912\n",
      "Epoch 21/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0154 - acc: 0.9947 - val_loss: 0.0410 - val_acc: 0.9910\n",
      "Epoch 22/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0417 - val_acc: 0.9904\n",
      "Epoch 23/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0161 - acc: 0.9947 - val_loss: 0.0427 - val_acc: 0.9898\n",
      "Epoch 24/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0462 - val_acc: 0.9899\n",
      "Epoch 25/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0126 - acc: 0.9959 - val_loss: 0.0422 - val_acc: 0.9908\n",
      "Epoch 26/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0148 - acc: 0.9952 - val_loss: 0.0454 - val_acc: 0.9902\n",
      "Epoch 27/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0132 - acc: 0.9958 - val_loss: 0.0466 - val_acc: 0.9892\n",
      "Epoch 28/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0141 - acc: 0.9952 - val_loss: 0.0492 - val_acc: 0.9902\n",
      "Epoch 29/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0116 - acc: 0.9958 - val_loss: 0.0436 - val_acc: 0.9900\n",
      "Epoch 30/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0506 - val_acc: 0.9902\n",
      "Epoch 31/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0485 - val_acc: 0.9907\n",
      "Epoch 32/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0119 - acc: 0.9960 - val_loss: 0.0447 - val_acc: 0.9906\n",
      "Epoch 33/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0115 - acc: 0.9963 - val_loss: 0.0504 - val_acc: 0.9902\n",
      "Epoch 34/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0131 - acc: 0.9957 - val_loss: 0.0460 - val_acc: 0.9908\n",
      "Epoch 35/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0459 - val_acc: 0.9905\n",
      "Epoch 36/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0126 - acc: 0.9962 - val_loss: 0.0434 - val_acc: 0.9910\n",
      "Epoch 37/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0446 - val_acc: 0.9908\n",
      "Epoch 38/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0510 - val_acc: 0.9908\n",
      "Epoch 39/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0097 - acc: 0.9967 - val_loss: 0.0529 - val_acc: 0.9900\n",
      "Epoch 40/50\n",
      "33600/33600 [==============================] - 3s 93us/sample - loss: 0.0107 - acc: 0.9967 - val_loss: 0.0472 - val_acc: 0.9913\n",
      "Epoch 41/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0106 - acc: 0.9966 - val_loss: 0.0457 - val_acc: 0.9907\n",
      "Epoch 42/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0511 - val_acc: 0.9910\n",
      "Epoch 43/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0102 - acc: 0.9968 - val_loss: 0.0488 - val_acc: 0.9908\n",
      "Epoch 44/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0564 - val_acc: 0.9895\n",
      "Epoch 45/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0514 - val_acc: 0.9902\n",
      "Epoch 46/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0098 - acc: 0.9967 - val_loss: 0.0535 - val_acc: 0.9904\n",
      "Epoch 47/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0097 - acc: 0.9965 - val_loss: 0.0461 - val_acc: 0.9910\n",
      "Epoch 48/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0526 - val_acc: 0.9905\n",
      "Epoch 49/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.0633 - val_acc: 0.9890\n",
      "Epoch 50/50\n",
      "33600/33600 [==============================] - 3s 92us/sample - loss: 0.0091 - acc: 0.9969 - val_loss: 0.0543 - val_acc: 0.9906\n",
      "Test loss: 0.05431537332541801\n",
      "Test accuracy: 0.9905952\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "kagglePred = model.predict(kaggleTest)\n",
    "results = np.argmax(kagglePred, axis=1)\n",
    "results = pd.Series(results, name=\"Label\")\n",
    "\n",
    "submission = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), results], axis=1)\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
